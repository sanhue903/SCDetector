{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4fc201ec",
      "metadata": {
        "id": "4fc201ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684812bf",
      "metadata": {
        "id": "684812bf"
      },
      "source": [
        "# CARGA DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc0ae272",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0ae272",
        "outputId": "d50077ec-6fb8-4b07-e88c-f463ec20bb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/data\"\n",
        "LABEL_MAP = {\"benign\": 0, \"malignant\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a925bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "1a925bb4",
        "outputId": "9abd90d0-6ece-484f-8214-9289a58f941d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f501d69333d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_set_images = []\n",
        "data_set_labels = []\n",
        "\n",
        "for label, value in LABEL_MAP.items():\n",
        "    label_path = os.path.join(DATA_DIR, label)\n",
        "\n",
        "    if not os.path.isdir(label_path):\n",
        "        print(f\"{label_path} (no es carpeta)\")\n",
        "        continue\n",
        "\n",
        "    i = 0\n",
        "    for file in os.listdir(label_path):\n",
        "        if i > 5000:\n",
        "            break\n",
        "        i += 1\n",
        "        if file.lower().endswith('.jpg'):\n",
        "            file_path = os.path.join(label_path, file)\n",
        "            image = cv2.imread(file_path)\n",
        "\n",
        "            if image is None:\n",
        "              print(f\"Error cargando {file_path}\")\n",
        "              continue\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            data_set_images.append(image)\n",
        "            data_set_labels.append(value)\n",
        "\n",
        "\n",
        "data_set_images = np.array(data_set_images)\n",
        "data_set_labels = np.array(data_set_labels)\n",
        "\n",
        "total_benign = np.sum(data_set_labels == LABEL_MAP[\"benign\"])\n",
        "\n",
        "print(f\"Total de datos cargados: {len(data_set_images)}\")\n",
        "print(f\"benigno: {total_benign}\")\n",
        "print(f\"malignano: {len(data_set_images) - total_benign}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6076614e",
      "metadata": {
        "id": "6076614e"
      },
      "source": [
        "Puesto que las clases tienen un tamaño bastante similar, no se harán tecnicas de balanceo de clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc168b9",
      "metadata": {
        "id": "7dc168b9"
      },
      "outputs": [],
      "source": [
        "def mostrar_muestras(imgs, lbls, n=5):\n",
        "    idxs = random.sample(range(len(imgs)), n)\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.imshow(imgs[idx])\n",
        "        etiqueta = \"Benigno\" if lbls[idx] == 0 else \"Malignano\"\n",
        "        plt.title(etiqueta)\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Muestras aleatorias del dataset\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "mostrar_muestras(data_set_images, data_set_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ec027e",
      "metadata": {
        "id": "a6ec027e"
      },
      "outputs": [],
      "source": [
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=DATA_DIR, transform=base_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5d87fb3",
      "metadata": {
        "id": "b5d87fb3"
      },
      "source": [
        "Cargamos la base de datos con ImageFolder, para que el entrenamiento consuma menos recurso, puesto que usando numpy array con imagenes pide demasiado RAM al entrenar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15deed2a",
      "metadata": {
        "id": "15deed2a"
      },
      "source": [
        "# SEPARACION DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2cd0ed",
      "metadata": {
        "id": "da2cd0ed"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9b7f83",
      "metadata": {
        "id": "db9b7f83"
      },
      "source": [
        "Dividimos la base de datos en 80% entrenamiento y 20% de testeo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2793cbcf",
      "metadata": {
        "id": "2793cbcf"
      },
      "source": [
        "# NORMALIZACIÓN DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243e3ce9",
      "metadata": {
        "id": "243e3ce9"
      },
      "outputs": [],
      "source": [
        "def compute_mean_std(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    total_images = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_images += batch_samples\n",
        "\n",
        "    mean /= total_images\n",
        "    std /= total_images\n",
        "    return mean, std\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "mean_train, std_train = compute_mean_std(train_loader)\n",
        "\n",
        "print(\"Mean (train):\", mean_train)\n",
        "print(\"Std (train):\", std_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3f8c1f",
      "metadata": {
        "id": "7a3f8c1f"
      },
      "source": [
        "Puesto que se esta utilizando ImageFolder, las imágenes no se cargan completamente en memoria, sino que se accede a ellas de forma diferida a través de un DataLoader. Esto significa que no podemos calcular directamente la media (mean) y desviación estándar (std) desde un arreglo de numpy. Entonces se debe calcular estos valores recorriendo las imágenes del conjunto de entrenamiento con un DataLoader.\n",
        "\n",
        "Una vez obtenidas la mean y std del set de entrenamiento, estas se utilizan para normalizar tanto las imágenes del entrenamiento como las del testeo. Esto es importante porque el modelo aprende a partir de datos normalizados con esas estadísticas, y necesita recibir la información en el mismo formato durante el testeo\n",
        "\n",
        "No se deben calcular mean y std utilizando todo el dataset (train + test), ya que eso implicaría tener acceso a información del conjunto de prueba durante el preprocesamiento.\n",
        "\n",
        "Eso es lo que se conoce como data leakage, y puede generar resultados artificialmente optimistas, porque el modelo estaría condicionado por datos que, en teoría, aún no debería conocer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f043c3f8",
      "metadata": {
        "id": "f043c3f8"
      },
      "source": [
        "# DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11a28e1",
      "metadata": {
        "id": "d11a28e1"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_train.tolist(), std_train.tolist())\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_train.tolist(), std_train.tolist())\n",
        "])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "test_dataset.dataset.transform = transform_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec1cf08",
      "metadata": {
        "id": "7ec1cf08"
      },
      "source": [
        "Se utilizó aumentación de datos para el entrenamiento para evitar overfitting. Para eso se hizo técnicas de rotación, flip y se varió el brillo y contraste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b8d46e",
      "metadata": {
        "id": "a9b8d46e"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"{len(train_dataset)} datos de entrenamiento - {len(test_dataset)} datos para testear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1c3c40",
      "metadata": {
        "id": "5b1c3c40"
      },
      "outputs": [],
      "source": [
        "class MLPDetector(nn.Module):\n",
        "  def __init__(self, features_n):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.hidden = nn.Linear(features_n, features_n//8)\n",
        "    self.output = nn.Linear(features_n//8, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.hidden(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = self.output(x)\n",
        "    x = nn.Sigmoid()(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71d7b2a",
      "metadata": {
        "id": "a71d7b2a"
      },
      "outputs": [],
      "source": [
        "class StepByStep(object):\n",
        "  def __init__(self, model, loss_fn, optimizer):\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.model.to(self.device)\n",
        "\n",
        "    self.train_loader = None\n",
        "    self.val_loader = None\n",
        "\n",
        "    self.losses = []\n",
        "    self.val_losses = []\n",
        "    self.total_epochs = 0\n",
        "\n",
        "    self.train_step_fn = self._make_train_step_fn()\n",
        "    self.val_step_fn = self._make_val_step_fn()\n",
        "    self.writer = None\n",
        "\n",
        "\n",
        "  def to(self, device):\n",
        "    try:\n",
        "      self.device = device\n",
        "      self.model.to(self.device)\n",
        "\n",
        "    except RuntimeError:\n",
        "      self.device = ('cuda' if torch.cuda.is_available()\n",
        "      else 'cpu')\n",
        "      print(f\"Couldn't send it to {device}, \\\n",
        "      sending it to {self.device} instead.\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "  def set_loaders(self, train_loader, val_loader):\n",
        "    self.train_loader = train_loader\n",
        "    self.val_loader = val_loader\n",
        "\n",
        "  def _make_train_step_fn(self):\n",
        "    def perform_train_step_fn(x, y):\n",
        "      self.model.train()\n",
        "      yhat = self.model(x)\n",
        "      loss = self.loss_fn(yhat, y.unsqueeze(1).float())\n",
        "      loss.backward()\n",
        "\n",
        "      self.optimizer.step()\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      return loss.item()\n",
        "\n",
        "    return perform_train_step_fn\n",
        "\n",
        "  def _make_val_step_fn(self):\n",
        "    def perform_val_step_fn(x, y):\n",
        "      self.model.eval()\n",
        "      yhat = self.model(x)\n",
        "      loss = self.loss_fn(yhat, y.unsqueeze(1).float())\n",
        "      return loss.item()\n",
        "\n",
        "    return perform_val_step_fn\n",
        "\n",
        "  def _mini_batch(self, validation=False):\n",
        "    if validation:\n",
        "      data_loader = self.val_loader\n",
        "      step_fn = self.val_step_fn\n",
        "    else:\n",
        "      data_loader = self.train_loader\n",
        "      step_fn = self.train_step_fn\n",
        "\n",
        "    if data_loader is None:\n",
        "      return None\n",
        "\n",
        "    mini_batch_losses = []\n",
        "    for x_batch, y_batch in data_loader:\n",
        "      x_batch = x_batch.to(self.device)\n",
        "      y_batch = y_batch.to(self.device)\n",
        "\n",
        "      mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "      mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "    loss = np.mean(mini_batch_losses)\n",
        "    return loss\n",
        "\n",
        "  def set_seed(self, seed=42):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "  def train(self, n_epochs, seed=42):\n",
        "    self.set_seed(seed)\n",
        "    for epoch in range(n_epochs):\n",
        "      self.total_epochs += 1\n",
        "      print(f\"Epoch {self.total_epochs}/{n_epochs}\")\n",
        "      loss = self._mini_batch(validation=False)\n",
        "      self.losses.append(loss)\n",
        "      with torch.no_grad():\n",
        "        # Performs evaluation using mini-batches\n",
        "        val_loss = self._mini_batch(validation=True)\n",
        "        self.val_losses.append(val_loss)\n",
        "      # If a SummaryWriter has been set...\n",
        "        if self.writer:\n",
        "          scalars = {'training': loss}\n",
        "          if val_loss is not None:\n",
        "            scalars.update({'validation': val_loss})\n",
        "\n",
        "          self.writer.add_scalars(main_tag='loss',\n",
        "                                  tag_scalar_dict=scalars,\n",
        "                                  global_step=epoch)\n",
        "      if self.writer:\n",
        "        self.writer.flush()\n",
        "  def save_checkpoint(self, filename):\n",
        "    checkpoint = {\n",
        "      'epoch': self.total_epochs,\n",
        "      'model_state_dict': self.model.state_dict(),\n",
        "      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "      'loss': self.losses,\n",
        "      'val_loss': self.val_losses\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, filename)\n",
        "  def load_checkpoint(self, filename):\n",
        "    checkpoint = torch.load(filename)\n",
        "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.optimizer.load_state_dict(\n",
        "      checkpoint['optimizer_state_dict']\n",
        "    )\n",
        "    self.total_epochs = checkpoint['epoch']\n",
        "    self.losses = checkpoint['loss']\n",
        "    self.val_losses = checkpoint['val_loss']\n",
        "    self.model.train()\n",
        "\n",
        "  def predict(self, x):\n",
        "    self.model.eval()\n",
        "    x_tensor = torch.as_tensor(x).float()\n",
        "    y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "    self.model.train()\n",
        "    return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "  def plot_losses(self):\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "    plt.plot(self.losses, label='Training Loss', c='b')\n",
        "\n",
        "    if self.val_loader:\n",
        "      plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5483b90",
      "metadata": {
        "id": "f5483b90"
      },
      "outputs": [],
      "source": [
        "model = MLPDetector(features_n=3*112*112)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "sbs = StepByStep(model, loss_fn, optimizer)\n",
        "sbs.set_loaders(train_loader, test_loader)\n",
        "#sbs.set_tensorboard('classy')\n",
        "\n",
        "sbs.set_loaders(train_loader, test_loader)\n",
        "sbs.train(n_epochs=100)\n",
        "sbs.plot_losses()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}